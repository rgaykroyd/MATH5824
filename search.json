[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH5824 Generalized Linear and Additive Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\n\n\n\n\n\n\nWeek 6 (6 - 10 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Sections 4.1-4.4.\nLecture on Tuesday: Complete Chapter 4: with Section 4.5 Fitting GLMs in R\nLecture on Thursday: Start Chapter 5: Logistic Regression.\nLecture on Friday: MATH5824 Chapter 4 Sections 4.5 Smoothing splines in R.\nWeekly feedback: Relevant exercise questions from last year (solutions online).\n\n\n\n\n\n\n\n\n\nWeek 5 (27 February - 3 March)\n\n\n\n\nBefore next Lecture: Re-read Chapter 4: Section 4.1.\nLecture on Tuesday: Cancelled due to illness. Please read Chapter 4: Section 4.2.\nLecture on Thursday: Chapter 4: Sections 4.3, 4.4 & 4.5.\nWeekly feedback: Start Exercises in Chapter 4.\n\n\n\n\n\n\n\n\n\nWeek 4 (20 - 24 February)\n\n\n\n\nBefore next Lecture: Be confident with all material up to, and including, Section 3.4 Moments of exponential-family distributions.\nLecture on Tuesday: Chapter 3: Sections 3.5 & 3.6\nLecture on Thursday: Start Chapter 4 by covering Section 4.1.\nWeekly feedback: Complete Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 3 (13 - 17 February)\n\n\n\n\nBefore next Lecture: Be confident with material in Chapter 2: Essentials of Normal Linear Models.\nLecture on Tuesday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.1 & 3.2.\nLecture on Thursday: Cancelled due to UCU strike. Instead, self-study Chapter 3: Sections 3.3 & 3.4.\nBefore next Lecture: Complete questions and check solutions, including video(s), for all Exercises in Chapters 1 and 2. Start Exercises in Chapter 3.\n\n\n\n\n\n\n\n\n\nWeek 2 (6 - 10 February)\n\n\n\n\nBefore next Lecture: Please re-read Section 2.1: Overview and read Section 2.2: Types of normal linear model.\nLecture on Tuesday: We will briefly cover all remaining material in Chapter 2: Essentials of Normal Linear Models.\nBefore next Lecture: Please re-read Chapter 2 carefully.\nLecture on Thursday: Cancelled due to UCU strike.\nWeekly feedback: Self-study the Exercises in Section 2.6 – solutions to be posted during Week 3.\n\n\n\n\n\n\n\n\n\nWeek 1 (30 January - 3 February)\n\n\n\n\nBefore next Lecture: Please read the Overview.\nLecture on Tuesday: We will briefly cover all material in Chapter 1: Introduction.\nBefore next Lecture: Please re-read Chapter 1 carefully.\nLecture on Thursday: Start Chapter 2: Essentials of Normal Linear Models with Section 2.1: Overview.\nWeekly feedback: Self-study the Exercises in Section 1.5 – solutions to be posted during Week 1.\n\n\n\n\n\n\n\n\n\nCoursework Practical Sessions (20 - 24 March)\n\n\n\n\nCoursework for this module involves a single written report worth 20% of the module grade. This will mainly involve investigating different models using R and interpreting the results.\nTasks are expected to be handed out on 14 March with hand-in deadline expect to be 31 March."
  },
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module “MATH5824 - Generalized Linear and Additive Models” for the academic year 2022-23. They are based on those used previously for this module and I am grateful to previous module lecturers for their considerable effort: Lanpeng Ji, Amanda Minter, John Kent, Wally Gilks, and Stuart Barber. This is the first year, however, that they have been produced in accessible format and hence some errors might occur during this conversion process. For information, I am using Quarto (a successor to RMarkdown) from RStudio to produce both the html and PDF, and then GitHub to create the website which can be accessed at rgaykroyd.github.io/MATH3823/. Please note that the PDF versions will only be made available on the University of Leeds Minerva system. Although I am a long-term user of RStudio, I have not previously used Quarto/RMarkdown nor Github and hence please be patient if there are hitches along the way.\nIn the Level 3 component of this module, we extend the simple linear regression model to the generalized linear model which can cope with non-normally distributed response variables, in particular data following binomial and Poisson distributions. However, we still just use linear functions of the predictor variables. A further extension of the linear model is the generalized additive model. Here, we no longer insist on the predictor variables affecting the response via a linear function of the predictors, but allow the response to depend on a more general smooth function of the predictor. In the Level 5 component of this module, we study splines and their use in interpolating and smoothing the effects of explanatory variables in the generalized linear models of the Level 3 component of this module (see separate Lecture Notes accompanying MATH3823).\nRG Aykroyd, Leeds, November 22, 2022\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is limited to normally distributed responses. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data.A further extension is the generalised additive model. Here, we no longer insist on the predictor variables affecting the response via a linear function of the predictors, but allow the response to depend on a more general smooth function of the predictor."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions, deviance and overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\ncompare a number of methods for scatterpot smoothing suitable for use in a generalised additive model;\nuse a backfitting algorithm to estimate the parameters of a generalised additive model;\ninterpret a fitted generalised additive model;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models; scatterplot smoothers; generalised additive model."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH5824 Module Catalogue page"
  },
  {
    "objectID": "1_introduction.html#motivation",
    "href": "1_introduction.html#motivation",
    "title": "1  Non-parametric Modelling",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nTable 1.1 reports on the depth of a coal seam determined by drilling bore holes at regular intervals along a line. The depth \\(y\\) at location \\(x=6\\) is missing: could we estimate it?\n\n\nTable 1.1: Coal-seam depths (in metres) below the land surface at intervals of 1 km along a linear transect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation, \\(x\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nDepth, \\(y\\)\n-90\n-95\n-140\n-120\n-100\n-75\nNA\n-130\n-110\n-105\n-50\n\n\n\n\nFigure 1.1 plots these data, superimposed with predictions from several polynomial regression models.\n\n\n\n\n\n\n\n(a) Constant model\n\n\n\n\n\n\n\n(b) Linear model\n\n\n\n\n\n\n\n\n\n(c) Quadratic model\n\n\n\n\n\n\n\n(d) Cubic model\n\n\n\n\nFigure 1.1: The coal-seam data superimposed with predictions from polynomial regression models.\n\n\nEach of these models would predict a different value for the missing observation \\(y_6.\\) We do not know the accuracy of the depth measurements, so in principle any of these curves could be correct. Clearly, the residual variance is largest for the constant-depth model in Figure 1.1 (a), and smallest for the cubic polynomial in Figure 1.1 (c). However, none of these models produces a convincingly good fit. Moreover, these models are not particularly believable, since we know that geological pressures exerted over very long periods of time cause the landscape and its underlying layers of rock to undulate and fracture. This suggests we need a different strategy.\nNext, consider the simulated example in Figure 1.2. At first look we might be happy with the fitted curves in Figure 1.2 (a) or Figure 1.2 (b). The data, however, are created with a change-point at \\(x=0.67\\) where the relationship changes from linear with slope \\(0.6\\) to a constant value of \\(0.75\\). This description is completely lost with these two models.\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Quadratic model\n\n\n\n\n\n\n\n\n\n(c) Piecewise linear model\n\n\n\n\n\n\n\n(d) Cubic smoothing spline\n\n\n\n\nFigure 1.2: Simulated data superimposed with predictions from various models.\n\n\nFigure 1.2 (c) shows the result of fitting one linear function to the data below \\(0.67\\) and a second linear function above. Clearly, this fits well but it has assumed that the change-point location is known – which is unrealistic. Finally, Figure 1.2 (d) shows a fitted cubic smoothing spline to the data – we will studies these models later. This shows an excellent fit and leads to appropriate conclusions. That is, the relationship is approximately linear for small values, then there is a rapid increase, and finally a near constant value for high values. Of course, this is not exactly as the true relationship with a discontinuity at \\(x=0.67\\) but it would definitely suggest something extreme occurs between about \\(0.6\\) to \\(0.7\\). Full details will follow later, but the cubic spline fits local cubic polynomials which are constrained to create a continuous curve.\nNow returning to the coal seam data. Figure 1.3 shows the data again, superimposed with predictions from methods which are not constrained to produce such smooth curves.\n\n\n\n\n\n\n\n(a) Constant interpolating spline\n\n\n\n\n\n\n\n(b) Linear interpolating spline\n\n\n\n\n\n\n\n\n\n(c) Cubic interpolating spline\n\n\n\n\n\n\n\n(d) Cubic smoothing spline\n\n\n\n\nFigure 1.3: The coal-seam data superimposed with predictions from various spline models.\n\n\nThe simplest method, constant-spline interpolation, assumes that the dependent variable remains constant between successive observations, with the result shown in Figure 1.3 (a). However, the discontinuities in this model make it quite unreliable. A better method, whose results are shown in Figure 1.3 (b), is linear-spline interpolation, which fits a straight line between successive observations. Even so, this method produces discontinuities in the gradient at each data point. A better method still, shown in Figure 1.3 (c), is cubic spline interpolation, which fits a cubic polynomial between successive data points such that both the gradient and the curvature at each data point is continuous.\nA feature of all these interpolation methods is that they fit the data exactly. Is this a good thing? The final method assumes that there may be some measurement error in the observations, which justifies fitting a smoother cubic spline than the cubic interpolating spline, but as we see in Figure 1.3 (d) which does not reproduce the data points exactly. Is this a bad thing? We will see during this module how to construct and evaluate these curves. Here, the results are presented only for motivation."
  },
  {
    "objectID": "1_introduction.html#sec-genmodelling",
    "href": "1_introduction.html#sec-genmodelling",
    "title": "1  Non-parametric Modelling",
    "section": "1.2 General modelling approaches",
    "text": "1.2 General modelling approaches\nWe wish to model the dependence of a response variable \\(y\\) on an explanatory variable \\(x\\), where \\(y\\) and \\(x\\) are both continuous. We observe \\(y_i\\) at each time \\(x_i,\\) for \\(i=1,\\ldots, n\\), where the observation locations are ordered: \\(x_1 < x_2 < \\ldots <x_n\\). We imagine that the \\(y\\)’s are noisy versions of a smooth function of \\(x\\), say \\(f(x)\\). That is, \\[\ny_i = f(x_i) + \\epsilon_i,\n\\tag{1.1}\\] where the \\(\\{\\epsilon_i\\}\\) are i.i.d: \\[\n\\epsilon_i \\sim \\mathrm{N}(0,\\sigma^2).\n\\tag{1.2}\\] We suppose we do not know the correct form of function \\(f\\): how can we estimate it?\nIt is useful to divide modelling approaches into two broad types: parametric and non-parametric.\n\nParametric models\nBy far the most common parametric model is simple linear regression, for example, \\(f(x) = \\alpha + \\beta x\\), where parameters \\(\\alpha\\) and \\(\\beta\\) are to be estimated. This is, of course, the simplest example of the polynomial model family, \\(f(x) = \\alpha + \\beta x + \\gamma x^2 +\\cdots + \\omega \\; x^p\\), where \\(p\\) is the order of the polynomial and where all of \\(\\alpha, \\beta, \\gamma, \\dots, \\omega\\) are to be estimated. This has as special cases: quadratic, cubic, quartic, and quintic polynomials models. Also common are exponential models, for example \\(f(x) = \\alpha e^{-\\beta x}\\), where \\(\\alpha,\\beta\\) are to be estimated – do not confuse this with the exponential probability density function.\nNote that the polynomial models are all linear functions of the parameters. They are standard forms in regression modelling, as studied in MATH3714 (Linear regression and Robustness) and MATH3823 (Generalized linear models). The exponential model, however, is an example of a model which is non-linearly in the parameters – it is an example of a non-linear regression model.\nAlthough very many parametric models exist, they are all somewhat inflexible in their description of \\(f\\). They cannot accommodate arbitrary fluctuations in \\(f(x)\\) over \\(x\\) because they contain only a small number of parameters (degrees-of-freedom).\n\n\nNon-parametric models\nIn such models, \\(f\\) is assumed to be a smooth function of \\(x\\), but otherwise we do not know what \\(f\\) looks like. A smooth function \\(f\\) is such that \\(f(x_i)\\) is close to \\(f(x_j)\\) whenever \\(x_i\\) is close to \\(x_j\\). To characterize and fit \\(f\\) we will use an approach based on splines. In practice, different approaches to characterizing and fitting smooth \\(f\\) lead to similar fits to the data. The spline approach fits neatly with normal and generalized linear models (NLMs and GLMs), but so do other approaches (for example, kernel smoothing and wavelets). Methods of fitting \\(f\\) based on kernel smoothing and the Nadaraya–Watson estimator are studied in the Level 5 component of MATH5714 (Linear regression, robustness and smoothing) where the choice of bandwidth in kernel methods is analogous to the choice of smoothing parameter value in spline smoothing.\n\n\nPiecewise polynomial models\nA common problem with low-order polynomials is that they can often fit well for part of the data but have unappealing features elsewhere. For example, although none of the models in Figure 1.1 fit the data at all well, we might imagine that three short linear segments might be a good fit to the coal-seam data. Also, the piecewise linear model was a good description of the data in Figure 1.2 (c). This suggests that local polynomial models might be useful. In some situation, for example when we know that the function \\(f\\) is continuous, jumps in the fitted model, as in Figure 1.2 (c), are unacceptable. Alternatively, we may require differentiability of \\(f\\). Such technical issues lead to the use of splines, which is introduced in the next chapter."
  },
  {
    "objectID": "2_basicdefinitions.html#basic-definitions",
    "href": "2_basicdefinitions.html#basic-definitions",
    "title": "2  Introducing Splines",
    "section": "2.1 Basic definitions",
    "text": "2.1 Basic definitions\nLet \\(t_1 < t_2 < \\ldots < t_m\\) be a fixed set of sites or knots which need not correspond to observation locations, as in Figure 2.1.\n\n\n\n\n\nFigure 2.1: Diagram of knots and data points.\n\n\n\n\nNote that we use the symbol \\(t\\), rather than \\(x\\), so that we do not confuse knots and observation locations.\nA spline of order \\(p\\geq 1\\) is a piecewise-polynomial of order \\(p\\) which is \\((p-1)\\) times differentiable at the knots. Thus there are coefficients \\(\\{a_{k\\ell},\\; k=0,\\ldots,m, \\; \\ell=0,\\ldots,p\\}\\) such that \\[\nf(t) = \\sum_{\\ell=0}^p a_{k\\ell} \\; t^\\ell,\\qquad \\text{for}~t_k \\leq t <t_{k+1},\n\\tag{2.1}\\] where we take \\(t_0 = -\\infty\\) and \\(t_{m+1} = +\\infty\\).\nIf we are using cubic polynomials, (\\(p=3\\)), then \\(f\\) is given by the following equations: \\[\nf(t) = a_{00} + a_{01} t + a_{02}t^2 +a_{03}t^3, \\quad\nt_0 \\le t < t_1\n\\] to the left of the first knot, \\[\nf(t) = a_{10} + a_{11} t + a_{12}t^2 +a_{13}t^3, \\quad\nt_1 \\le t < t_2\n\\] between the first and second knots, and so on until \\[\nf(t) = a_{m0} + a_{m1} t + a_{m2}t^2 +a_{m3}t^3, \\quad\nt_m \\le t < t_{m+1}\n\\] to the right of the final knot. This is illustrated in Figure 2.2 (a) with \\(m=2.\\)\n\n\n\n\n\n\n\n(a) No smoothness constraints\n\n\n\n\n\n\n\n(b) With smoothness constraints\n\n\n\n\nFigure 2.2: Piecewise-cubic functions in three intervals with knot positions indicated with vertical lines.\n\n\nBecause of the use of polynomials, \\(f\\) is smooth between each successive pair of knots. At the knots, however, \\(f\\) might not be continuous and it might not be differentiable – in such cases we would say that the function is not smooth.\nTo ensure that \\(f\\) is also smooth at each of the knots, we impose smoothness constraints which control continuity of the function and its derivatives at the knots.\nLet \\(f^{(\\ell)}\\) be the \\(\\ell\\)-th order derivative, with \\(f^{(0)}=f\\) being the function itself, \\(f^{(1)}=f'\\) is the first derivative and \\(f^{(2)}=f''\\) the second derivative. Further, let \\(f^{(\\ell)}(t-\\epsilon)\\) and \\(f^{(\\ell)}(t+\\epsilon)\\), for \\(\\epsilon\\ge 0\\), denote evaluation of the function or its derivative at points just below and just above \\(t\\) – we will be interested in their relative values as \\(\\epsilon \\rightarrow 0\\).\nTo impose smoothness, we require that\n\\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_k-\\epsilon)\n=\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_k+\\epsilon),\n\\tag{2.2}\\] for all \\(k=1,\\ldots,m\\) and for \\(\\ell = 0,\\ldots,(p-1)\\).\nIn other words we say that \\(f\\) is smooth if the limits, from below and from above, of the function and its \\((p-1)\\) derivatives exist and are equal.\nThe meaning of these smoothness constraints is illustrated in Figure 2.2. In Figure 2.2 (a), a piecewise cubic function with two knots has been plotted. The first derivative, \\(f'\\), is discontinuous at the first knot and the function itself, \\(f\\), is discontinuous at the second knot. Figure 2.2 (b) shows a similar shaped cubic spline with two knots. This time, the function \\(f\\) and its first two derivatives are continuous at both knots.\nThe smoothness conditions in Equation 2.2 induce constraints on the coefficients \\(\\{a_{k\\ell}\\}\\). A polynomial of order \\(p\\) has \\(p+1\\) coefficients, and there are \\(m+1\\) intervals when we have \\(m\\) knots. This leads to \\((p+1)\\times(m+1)\\) coefficients but there are \\(p\\) constraints at each of the \\(m\\) knots. Thus the total degrees of freedom of the system is \\[\n\\text{df}_{\\text{spline}}   = (p+1)(m+1) - pm = m+p+1.\n\\tag{2.3}\\]\nThese degrees of freedom provide the necessary flexibility in the spline.\nNote that \\(f\\) is infinitely differentiable everywhere, except at the knots where it is \\(p-1\\) times differentiable. In particular, for \\(p=1\\), \\(f\\) is a linear spline comprising linear pieces constrained to be continuous at the knots, although the slope of \\(f\\) is discontinuous at the knots. Also, for \\(p=3\\), \\(f\\) is a cubic spline comprising cubic polynomial pieces continuous at the knots; where the first and second derivatives of \\(f\\) are also continuous, but the third derivative is discontinuous at the knots."
  },
  {
    "objectID": "2_basicdefinitions.html#sec-exercises2",
    "href": "2_basicdefinitions.html#sec-exercises2",
    "title": "2  Introducing Splines",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\n\n2.1 Why is it not sensible to define a smooth function made-up of constant components? Similarly, why is not sensible to create a differentiable function from linear splines?\n2.2 In the situation illustrated in Figure 2.2 (b), where \\(p=3\\) and \\(m=2\\), clearly identify the \\((p+1)\\times (m+1)=12\\) model parameters and the \\(pm=6\\) smoothness constraints in terms of the cubic polynomials and their derivatives.\n2.3 Further consider the situation illustrated in Figure 2.2 (b). Suppose now that we require the splines to pass through specified coordinates \\((t_1, f(t_1))\\) and \\((t_2, f(t_2))\\). What is the degrees of freedom for this model? How many such cubic splines would satisfy these constraints? Discuss potential additional constraints which would lead to a unique fitted model. Do you think having a unique solution is a positive or negative property?\n2.4 For a general problem, what would be the effect of requiring additional constraints of the form of Equation 2.2 but with \\(\\ell = p\\)? Would this lead to an acceptable fitted cubic spline model? Justify your answer.\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 2.2 Solutions can be found here."
  },
  {
    "objectID": "3_interpolating.html#overview",
    "href": "3_interpolating.html#overview",
    "title": "3  Interpolating Splines",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nChapter 1 considered general limitations of parametric models, and polynomial regression in particular (see Figure 1.1), which motivated the use of the more flexible spline models (see Figure 1.3) – though at that stage no mathematical details were presented. In Chapter 2, basic spline definitions were given, including the notation of smoothness constraints, and these ideas were further explored in the Exercises in Section 3.6. This chapter will now give mathematical details of the interpolating spline problem and consider application to data. A feature of all these interpolation methods is that they fit the data exactly and that the fitted functions are smooth. Figure 3.1, is cubic spline interpolation, which fits a cubic polynomial between successive data points such that the function, gradient and the curvature are all continuous at each data point. The solid line shows the fitted values within the range of the data, whereas the dashed line shows the fitted values outside the range of the data – extrapolation.\n\n\nCode\npar(mar=c(3,3,1,1), mgp=c(2,1,0))\n\nlocation=0:10\ndepth=c(-90,-95,-140,-120,-100,-75,NA,-130,-110,-105,-50)\n\nplot(location, depth, \n     xlim=c(-0.5,10.5), ylim=c(-150,0), pch=4)\n\nmysplinefit = splinefun(location, depth, method=\"natural\")\ncurve(mysplinefit,-0.75,10.5, add=T, lty=2)\ncurve(mysplinefit,   0,   10, add=T)\n\n\n\n\n\nFigure 3.1: A cubic interpolating spline fitted to the coal-seam data, with the dashed line showing extrapolation."
  },
  {
    "objectID": "3_interpolating.html#natural-splines",
    "href": "3_interpolating.html#natural-splines",
    "title": "3  Interpolating Splines",
    "section": "3.2 Natural splines",
    "text": "3.2 Natural splines\nSuppose we have \\(n\\) observations \\(\\{y_1,\\dots,y_m\\}\\) at locations \\(\\{t_1,\\dots,x_m\\}\\). We can construct a cubic spline (that is with \\(p=3\\)) to pass through (interpolate) all the points \\((t_i,y_i),\\ i=1,\\dots,m\\). In fact, for any given set of points, there is an infinite number of cubic splines which interpolate them, see Figure 3.2 for examples. Exactly one of these splines has the property that, in the leftmost and rightmost intervals, it is a straight line. Such a spline is called a natural cubic spline.\n\n\n\n\n\nFigure 3.2: Cubic interpolating splines fitted to the coal-seam data, with the dashed lines showing extrapolation – the natural spline is shown in solid black."
  },
  {
    "objectID": "3_interpolating.html#properties-of-natural-splines",
    "href": "3_interpolating.html#properties-of-natural-splines",
    "title": "3  Interpolating Splines",
    "section": "3.3 Properties of natural splines",
    "text": "3.3 Properties of natural splines\nNatural splines are a special case of polynomial splines of odd order \\(p\\). Thus we have natural linear splines (\\(p=1\\)), natural cubic splines (\\(p=3\\)), etc. A spline is said to be natural if, beyond the boundary knots \\(t_1\\) and \\(t_{m}\\), its \\((p+1)/2\\) higher-order derivatives are zero: \\[\nf^{(j)}(t) = 0,  \n\\tag{3.1}\\] for \\(j={(p+1)}/{2},\\ldots,p\\) and either \\(t \\leq t_1\\) or \\(t \\geq t_m\\).\nThus a natural spline of order \\(p\\) has the following \\(p+1\\) constraints, in addition to those of Equation 2.2 : \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_1-\\epsilon)\n=\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_m+\\epsilon) =0,\n\\tag{3.2}\\] for \\(\\ell={(p+1)}/{2},\\ldots,p\\).\nIn particular,\n\na natural linear spline has \\(p+1=2\\) additional constraints: \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(1)}(t_1-\\epsilon)\n=\n\\lim _{\\epsilon\\rightarrow 0} f^{(1)}(t_m+\\epsilon) =0,\n\\tag{3.3}\\] implying that \\(f(t)\\) is constant in the outer intervals of a natural linear spline,\na natural cubic spline has \\(p+1=4\\) additional constraints: \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(2)}(t_1-\\epsilon) = \\lim _{\\epsilon\\rightarrow 0} f^{(2)}(t_m+\\epsilon) =0,\n\\] \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(3)}(t_1-\\epsilon) = \\lim _{\\epsilon\\rightarrow 0} f^{(3)}(t_m+\\epsilon) =0,\n\\tag{3.4}\\] implying that \\(f(t)\\) is linear in the outer intervals of a natural cubic spline.\n\nThe total degrees of freedom of a natural spline is, starting from Equation 2.3, but taking into account the additional \\(p+1\\) additional constraints is \\[\n\\text{df}_{\\text{nat.spline}} = m+p+1 - (p+1) = m.\n\\tag{3.5}\\] That is the degrees of freedom for natural splines equals \\(m\\) whatever the value of \\(p\\).\n\nProposition 3.1 Linear and cubic natural splines have the following representations:\n\nLinear natural splines: \\[\nf(t) = a_0 + \\sum_{i=1}^m b_i \\left| t - t_i \\right|;  \\quad  \\sum_{i=1}^m b_i = 0\n\\tag{3.6}\\]\nCubic natural splines: \\[\nf(t) = a_0 + a_1 t + \\sum_{i=1}^m b_i \\left| t - t_i \\right|^3;  \\quad  \\sum_{i=1}^m b_i = \\sum_{i=1}^m b_i t_i = 0.\n\\tag{3.7}\\]\n\n\nProof: Not covered here (but may be included later in the module if time allows)."
  },
  {
    "objectID": "3_interpolating.html#roughness-penalties",
    "href": "3_interpolating.html#roughness-penalties",
    "title": "3  Interpolating Splines",
    "section": "3.4 Roughness penalties",
    "text": "3.4 Roughness penalties\nAn aim of spline models is to describe an unknown function using piecewise-polynomials which are smooth. In the previous section, smoothness was imposed by explicitly constraining specified high-order derivatives. An alternative approach is to measure and control the degree of smoothness of the splines. In practice the roughness of the spline is usually measured and one definition of roughness is: \\[\nJ_\\nu(f) = \\int_{-\\infty}^\\infty \\left[ f^{(\\nu)}(t) \\right]^2 \\text{d}t\n\\tag{3.8}\\] where \\(\\nu\\geq 1\\) is an integer and \\(f^{(\\nu)}\\) denotes the \\(\\nu\\)th derivative of \\(f\\). Thus \\(f^{(1)}(t)\\) denotes the first derivative and \\(f^{(2)}(t)\\) denotes the second derivative of \\(f\\).\nIntuitively, roughness measures the “wiggliness” of a function.\nAim might be to find the smoothest function which interpolates the data points. Hence, an alternative approach to that in previous sections is to find the function \\(f\\) which minimizes Equation 3.8 and satisfies \\(f(t_i)=y_i\\) for \\(i=1,\\dots,m\\). We refer to the solutions of this problem as the optimal interpolating function.\nIt turns out that there is a very close link between \\(J_\\nu(\\cdot)\\) and \\(p\\)th-order natural splines, where \\(p=2\\nu-1\\) (so \\(p\\) is odd). Important special cases are: \\(\\nu=1\\) and \\(p=1\\), and \\(\\nu=2\\) and \\(p=3\\). This relationship is defined in the following proposition.\n\nProposition 3.2 The optimal interpolating function is a \\(p\\)th-order natural spline, where \\(p = 2ν − 1\\). That is, the natural spline \\(f\\) is the unique minimizer of \\(J_\\nu(f)\\).\n\nProof: Not covered here (but may be included later in the module if time allows).\n\nComments\n\nLinear and cubic interpolating splines are also of interest in numerical analysis, for example to interpolate tables of numbers.\nThe linear interpolating spline is simply the piecewise-linear path connecting the data points.\nOf course, in the linear spline case, knot points are clearly visible as kinks in the interpolating function.\nBut, in the cubic spline case, knots points are invisible to the naked eye. Hence, in general, there is little motivation to use higher-order splines.\nNumerical considerations: the interpolating spline solutions involve matrix inversion. The inversion of an \\(n \\times n\\) matrix involves \\(O(n^3)\\) operations – hence it is time consuming if \\(n\\) is large (for example, \\(n=1000\\) or \\(10000\\)). Fortunately there are tricks to reduce the computation to \\(O(n)\\)."
  },
  {
    "objectID": "3_interpolating.html#fitting-interpolating-splines-in-r",
    "href": "3_interpolating.html#fitting-interpolating-splines-in-r",
    "title": "3  Interpolating Splines",
    "section": "3.5 Fitting interpolating splines in R",
    "text": "3.5 Fitting interpolating splines in R\nThere are two main function within \\(\\mathbf{R}\\) for fitting interpolating splines to data, which outputs fitted values for specified points or which returns an \\(\\mathbf{R}\\) function which can be used directly by other commands, such as . The following illustrates the two approaches.\n\n\nCode\nset.seed(15342)\n\npar(mar=c(3,3,1,1), mgp=c(2,1,0))\n\nx = 1:9; y = rnorm(9)\n\n# spline deals with points\nplot(x, y, xlim=c(0,10), ylim=c(-3,3), pch=4)\n\nmysplinefit1 = spline(x, y, method=\"natural\")\nlines(mysplinefit1)\n\n\n# splinefun produces a function\nplot(x, y, xlim=c(0,10), ylim=c(-3,3), pch=4)\n\nmysplinefit2 = splinefun(x, y, method=\"natural\")\ncurve(mysplinefit2, 0, 10, add=T)\n\n\n\n\n\n\n\n\n(a) Using the spline command\n\n\n\n\n\n\n\n(b) Using the splinefun command\n\n\n\n\nFigure 3.3: R code for cubic interpolating splines.\n\n\n\nThe following, illustrates the different ways to draw the spline and to calculated fitted values.\n\n\nCode\nset.seed(15342)\n\nx = 1:9; y = rnorm(9)\n\n# spline deals with points\nmysplinefit1 = spline(x, y, method=\"natural\")\nspline(x, y, xout=c(2.5, 7.5), method=\"natural\")\n\n\n$x\n[1] 2.5 7.5\n\n$y\n[1]  0.08785792 -0.78655273\n\n\nCode\n# splinefun produces a function\nmysplinefit2 = splinefun(x, y, method=\"natural\")\nmysplinefit2(c(2.5, 7.5))\n\n\n[1]  0.08785792 -0.78655273"
  },
  {
    "objectID": "3_interpolating.html#sec-exercises2",
    "href": "3_interpolating.html#sec-exercises2",
    "title": "3  Interpolating Splines",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\n\nFurther questions will be added later.\n3.1 For the situation shown in Figure 2.2, but taking \\(p=1\\), write-down the linear functions for the three intervals and clearly identify all the \\(6\\) model parameters. Next, write down the constraints required to make the functions pass through the \\(m=2\\) data points, and the two constraints which impose continuity of function. What additional constraints are needed to fix the first derivative at zero for the outer two intervals?\n3.2 Continuing the problem described in Exercise 3.1, write the constraints as a system of 6 linear equations in the 6 unknown model parameters. How might you solve this system to give the parameter values which solve the interpolation problem?\n3.3 Continuing the linear system described in Exercise 3.2, create a synthetic problem by choosing two data response values. Then solve the system in \\({\\mathbf R}\\), or otherwise, and plot the fitted spline interpolating function.\n3.4 Again, considering the situation shown in Figure 2.2, but taking \\(p=1\\). Using the alternative representation in Equation 3.6, write down two constraints involving the data points and the additional constraint on the \\(b_i\\) parameters. Write this linear system of 3 equations in three unknowns in matrix form.\n3.5 Continuing the linear system described in Exercise 3.4, using the same points created in Exercise 3.4, calculate the parameter values in this new parameterization. Check that your two fitted interpolating spline give the same answers. Which approach do you prefer? Justify you answer.\n3.6 Create you own version of the R code used to produce Figure 3.3 and experiment with the two alternative spline fitting commands. Remove the \\(\\texttt{set.seed(15342)}\\) command so that you produce different data each time."
  },
  {
    "objectID": "4_smoothing.html#overview",
    "href": "4_smoothing.html#overview",
    "title": "4  Smoothing Splines",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nIn Section 1.2 we described a general statistical model with a response variable \\(y\\) and an explanatory variable \\(x\\). We observe \\(y_i\\) at each location \\(t_i,\\) for \\(i=1,\\ldots, n\\). We imagined that the \\(y\\)’s are noisy versions of a smooth function of \\(t\\), say \\(f(\\cdot)\\) where the errors follow a normal distribution with constant variance. That is \\[\ny_i = f(t_i) + \\epsilon_i, \\quad\n\\epsilon_i \\sim \\mathrm{N}(0,\\sigma^2),\n\\] for \\(i=1,\\ldots,n\\), where \\(f\\) is smooth, the \\(\\epsilon_i\\) are i.i.d., and \\(f\\) and \\(\\sigma^2\\) are unknown.\nThe log-likelihood for this situation is: \\[\nl(f; \\mathbf{y})\n= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - f(t_i)\\right)^2 - n \\log \\sigma\n\\tag{4.1}\\] and we wish to estimate \\(f\\) for a given data set \\(\\mathbf{y}=\\{y_1,\\dots, y_n\\}\\). With no constraints on \\(f\\), the log-likelihood would be maximized by setting \\(f(t_i)=y_i\\) for all \\(i\\), and we would estimate the noise variance as \\(\\hat{\\sigma}^2=0\\). This takes no account of randomness in the data and \\(f\\) would in general need to be quite wiggly to achieve this fit.\nFigure 4.1 (a) shows such an interpolation of noisy data. This would be of little use for explanation, interpolation or prediction.\n\n\n\n\n\n\n\n(a) Interpolating spline\n\n\n\n\n\n\n\n(b) Smoothing spline\n\n\n\n\nFigure 4.1: Comparison of interpolating and smoothing methods applied to a noisy dat set.\n\n\nWe do not expect, or even want, the fitted function \\(f\\) to pass exactly through the data points \\(\\{(t_i,y_i)\\}\\), but merely to lie close to them. We would rather trade-off goodness-of-fit against smoothness of \\(f\\). Figure 4.1 (b) shows a smoothing spline fit to the same data. This is much better as there is a clear explanation of the relationship, it could be used reasonably well for interpolation and prediction."
  },
  {
    "objectID": "4_smoothing.html#the-penalized-least-squares-criterion",
    "href": "4_smoothing.html#the-penalized-least-squares-criterion",
    "title": "4  Smoothing Splines",
    "section": "4.2 The penalized least-squares criterion",
    "text": "4.2 The penalized least-squares criterion\nNoting that maximizing Equation 4.1 is equivalent to minimizing the residual sum of squares: \\(\\sum_{i=1}^n \\left(y_i - f(t_i)\\right)^2\\), we can achieve this trade-off by minimizing a penalized sum of squared residuals: \\[\nR_\\nu(f,\\lambda) = \\sum_{i=1}^n \\left(y_i - f(t_i)\\right)^2 + \\lambda J_\\nu(f),\n\\tag{4.2}\\] where \\(J_\\nu(f)\\), as first defined in Equation 3.8, penalizes the roughness of \\(f\\). The smoothing parameter \\(\\lambda\\geq 0\\) controls the severity of this penalty. For now we will assume \\(\\lambda\\), which absorbs the \\(\\sigma^2\\) in Equation 4.1, is known.\nFigure 4.2 shows example smoothing spline fits using a range of smoothing parameters, \\(\\lambda\\). In Figure 4.2 (a) the fit is essentially a straight line, perhaps Figure 4.2 (b) and Figure 4.2 (c) show acceptable fits. Figure 4.2 (d), with a very small \\(\\lambda\\) value is close to an interpolating spline fit and is clearly unacceptable.\n\n\n\n\n\n\n\n(a) Very strong smoothing\n\n\n\n\n\n\n\n(b) Moderate smoothing\n\n\n\n\n\n\n\n\n\n(c) Weak smoothing\n\n\n\n\n\n\n\n(d) Very weak smoothing\n\n\n\n\nFigure 4.2: Comparison of interpolating and smoothing methods applied to a noisy dat set.\n\n\nAs the smoothing parameter \\(\\lambda\\) increases, the optimal \\(f\\) becomes smoother. In particular, it can be shown that as \\(\\lambda \\rightarrow \\infty\\), the vector of coefficients \\({\\mathbf b} \\rightarrow {\\mathbf 0}\\), and \\({\\mathbf a}\\) tends to the OLS estimate of the regression parameters. Thus, the smoothing spline converges to the sample mean \\(f(t)=\\bar y\\) when \\(\\nu=1\\), and to the ordinary least squares fitted line, \\(f(t)=\\hat\\alpha+\\hat\\beta t\\), when \\(\\nu=2\\). In the other direction, as \\(\\lambda \\rightarrow 0\\) the smoothing solution converges to the interpolating spline."
  },
  {
    "objectID": "4_smoothing.html#sec:relationto",
    "href": "4_smoothing.html#sec:relationto",
    "title": "4  Smoothing Splines",
    "section": "4.3 Relation to interpolating splines",
    "text": "4.3 Relation to interpolating splines\nWe show in the following proposition that the function \\(f\\) which minimizes Equation 4.2 is the interpolating spline of its fitted values.\n\nProposition 4.1 Suppose \\(\\hat{f}\\) minimizes \\(R_\\nu(f,\\lambda)\\) and let \\(\\hat{y}_i = \\hat{f}(t_i), i=1,\\dots,n\\) denote the corresponding fitted values. Then, \\(\\hat{f}\\) solves the interpolation problem for the artificial data set \\((t_i,\\hat{y}_i), i=1,\\dots , n\\). That is, \\(\\hat{f}\\) minimizes \\(J_\\nu(f)\\) over functions \\(f\\) satisfying \\(\\hat f(t_i)=\\hat{y}_i\\), \\(i=1,\\ldots,n\\). Consequently, \\(\\hat{f}\\) is a \\(p^{\\text{th}}\\)-order natural spline, where \\(p=2\\nu-1\\). This means that, when solving the smoothing problem, we only need consider spline functions given by the representations in Proposition 3.1.\n\n\n\n\n\n\n\n\n(a) Smoothing spline of data shown as dots and with fitted values shown as crosses\n\n\n\n\n\n\n\n(b) Interpolating spline of fitted values from smoothing problem\n\n\n\n\nFigure 4.3: Illustration of proposition showing that solution of the smothing problem is a natural interpolating spline.\n\n\nProof: Suppose the assertion is not true. In this case, we must be able to find a function \\({\\hat{f}^\\star}\\), say, which also interpolates the artificial data \\((t_i,\\hat{y}_i), i=1,\\dots ,n\\), but which has a smaller roughness penalty. That is \\[\nJ_\\nu({\\hat{f}^\\star}) < J_\\nu(\\hat{f})\n  \\mbox{ with }\n\\hat{f}^\\star(t_i) = \\hat{y}_i, \\ i=1,\\ldots,n.\n\\] Note that the fitted values from the function \\(\\hat f^\\star\\) are also equal to \\(\\hat y_i, i=1,\\dots,n\\) as it interpolates the same artificial data as \\(\\hat f\\).\nNow, from Equation 4.2, \\[\\begin{align*}\n  R_\\nu(\\hat{f}^\\star, \\lambda) &= \\sum_i (y_i-\\hat{y}_i)^2 + \\lambda J_\\nu(\\hat{f}^\\star)\\\\\n&< \\sum_i (y_i-\\hat{y}_i)^2 + \\lambda J_\\nu(\\hat{f})\n= R_\\nu(\\hat{f}, \\lambda).\n\\end{align*}\\] Hence \\(R_\\nu(\\hat{f}^\\star, \\lambda) < R_\\nu(\\hat{f}, \\lambda)\\). But, by construction \\(\\hat{f}\\) minimizes \\(R_\\nu(f, \\lambda)\\), which is a contradiction. Hence, it must not be possible to find a function \\({\\hat{f}^\\star}\\) which also interpolates the artificial data but which has a smaller roughness penalty.\nWe have shown that \\(\\hat{f}\\) is the optimal interpolant of the fitted values \\(\\hat{y}_i,\\ i=1,\\dots,n,\\) so it follows from Proposition 4.1 that \\(\\hat{f}\\) is a natural spline of order \\(p=2\\nu-1\\)."
  },
  {
    "objectID": "4_smoothing.html#sec-smoothingmatrix",
    "href": "4_smoothing.html#sec-smoothingmatrix",
    "title": "4  Smoothing Splines",
    "section": "4.4 The smoothing problem in matrix notation",
    "text": "4.4 The smoothing problem in matrix notation\nWe have just proved that the function \\(\\hat{f}\\) that minimises \\(R_\\nu (f, \\lambda)\\) must be a natural spline (linear if \\(\\nu=1\\), cubic if \\(\\nu=2\\)) with knots at \\(\\{t_i,\\,i=1,\\ldots,n\\}.\\) That is, as in Proposition 3.1, we can write:\n\\[\n\\hat{f}(t) = \\sum_{i=1}^n b_i\\left|t-t_i\\right|^p +\n\\begin{cases}\na_0, &\\nu=1\\\\\na_o + a_1t, &\\nu=2,\n\\end{cases}  \n\\tag{4.3}\\] where \\(p=2\\nu-1\\) and constraints \\[\n\\sum_i b_i = 0 \\, \\mbox{ for }\\, \\nu=1 \\quad\n\\sum_i b_i = \\sum_i b_i \\, t_i = 0\n\\, \\mbox{ for }\\, \\nu=2.\n\\tag{4.4}\\]\nHowever, we have not yet figured out how to calculate the parameter values \\(\\hat{a}_0,\\dots,\\hat{a}_{\\nu-1},\\ \\hat{b}_1,\\dots,\\hat{b}_n\\) in Equation 4.3 which optimally fit the data \\(y_1,\\dots,y_n\\). For this, it is convenient to re-express the penalized sum of squared residuals Equation 4.2 in matrix notation.\n\nProposition 4.2 The roughness of a natural linear spline (\\(\\nu=1\\), i.e. \\(p=1\\)) is \\[\nJ_1(f)  \n= -2\\sum_{i=1}^n \\sum_{k=1}^n b_i b_k \\left| t_i - t_k \\right|\n= c_1\\,{\\mathbf b}^T \\, K_1 \\, {\\mathbf b},\n\\tag{4.5}\\] and the roughness of a natural cubic spline (\\(\\nu=2\\), i.e. \\(p=3\\)) is \\[\nJ_2(f)  = 12\\sum_{i=1}^n \\sum_{k=1}^n b_i b_k \\left| t_i - t_k \\right|^3\n= c_2\\,{\\mathbf b}^T \\, K_2 \\, {\\mathbf b},\n\\tag{4.6}\\] where the constants are given by \\[\nc_1=-2, \\quad c_2=12.\n\\tag{4.7}\\] Here \\({\\mathbf b}=(b_1,\\ldots,b_n)^{T}\\) is the vector of spline coefficients and \\(K_\\nu\\) is the \\(n\\times n\\) matrix whose \\((i,k)\\)th element is \\(\\left| t_i - t_k \\right|^p\\), where \\(p=2\\nu-1\\) that is \\[\nK_\\nu\n=\n\\begin{bmatrix}\n|t_1-t_1|^p & |t_1-t_2|^p & \\dots & |t_1-t_n|^p \\\\\n|t_2-t_1|^p & |t_2-t_2|^p & \\dots & |t_2-t_n|^p \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n|t_n-t_1|^p & |t_n-t_2|^p & \\dots & |t_n-t_n|^p.\n\\end{bmatrix}\n\\]\n\nProof: To be covered later if there is sufficient time.\nFrom Proposition 4.2 the roughness penalty satisfies: \\[\\begin{align}\nJ_\\nu  (\\hat{f}) = c_\\nu   \\, {\\mathbf b}^T K_\\nu   {\\mathbf b} \\label{eq:Jnufhat}\n\\end{align}\\] where \\(c_1=-2\\); \\(c_2= 12\\); \\({\\mathbf b}=(b_1,\\ldots,b_n)^T\\); and \\(K_\\nu\\) is the \\(n \\times n\\) matrix whose \\((i,k)\\)th element is \\(\\left|t_i-t_k\\right|^p\\).\nFrom Equation 4.3, the value of \\(\\hat{f}\\) at knot \\(t_k\\) is \\[\n\\hat{f}(t_k) = \\sum_{i=1}^n b_i\\left|t_k-t_i\\right|^p + \\begin{cases}a_0, &\\nu=1\\\\ a_0 + a_1t_k, &\\nu=2 \\end{cases}.\n\\tag{4.8}\\]\nIn matrix form, this is \\[\n\\mathbf{\\hat{f}} = \\left[\n\\begin{array}{c}\\hat{f}(t_1) \\\\\n\\vdots  \\\\\n\\hat{f}(t_n)\\end{array}\n\\right]\n= K_\\nu   {\\mathbf b} + L_\\nu   \\mathbf{a}_{\\,\\nu}\n\\tag{4.9}\\] where \\[\nL_1 =\n\\left[\n\\begin{array}{c} 1\\\\ \\vdots \\\\ 1\\end{array}\\right], \\quad \\mathbf{a}_1 = a_0,\n\\quad \\quad \\mbox{and} \\quad \\quad\nL_2 =\n\\left[\n\\begin{array}{cc} 1 & t_1 \\\\\n\\vdots & \\vdots\n\\\\ 1 & t_n\n\\end{array}\n\\right],\n\\quad \\mathbf{a}_2 = \\left[\\begin{array}{l}a_0\\\\a_1\\end{array}\n\\right] .\n\\]\nThus, from Equation 4.4 – Equation 4.9, the penalized least-squares criterion Equation 4.2 reduces to a quadratic function of the parameters \\(\\mathbf{a}\\) and \\({\\mathbf b}\\):\n\\[\nR_\\nu(f, \\lambda) = (\\mathbf{y} - K_\\nu  \\, {\\mathbf b} - L_\\nu   \\, \\mathbf{a})^T  (\\mathbf{y} - K_\\nu  \\, {\\mathbf b} - L_\\nu   \\, \\mathbf{a})\n                            + \\lambda \\,c_\\nu  \\,{\\mathbf b}^T K_\\nu   \\,{\\mathbf b}\n\\tag{4.10}\\] subject to \\[\nL_\\nu^T\\,{\\mathbf b} = 0,\n\\tag{4.11}\\] where \\(\\mathbf{y} = (y_1,\\ldots,y_n)^T\\).\nTo find the explicit values for \\(\\mathbf{b}\\) and \\(\\mathbf{a}\\), we must minimize the quadratic function Equation 4.10 subject to the linear constraints Equation 4.11.\n\nProposition 4.3 The solution to the smoothing spline problem is given by \\[\n\\begin{bmatrix}\n\\hat{\\mathbf a} \\\\ \\hat{\\mathbf b}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & L_\\nu \\\\\nL_\\nu^T & K_\\nu+\\lambda^* I_n\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n{\\mathbf 0} \\\\ {\\mathbf y}\n\\end{bmatrix}\n\\tag{4.12}\\] and \\(\\lambda^* = c_\\nu \\lambda\\).\n\nProof: Omitted."
  },
  {
    "objectID": "4_smoothing.html#smoothing-splines-in-r",
    "href": "4_smoothing.html#smoothing-splines-in-r",
    "title": "4  Smoothing Splines",
    "section": "4.5 Smoothing splines in R",
    "text": "4.5 Smoothing splines in R\nNotes to be added"
  },
  {
    "objectID": "4_smoothing.html#exercies",
    "href": "4_smoothing.html#exercies",
    "title": "4  Smoothing Splines",
    "section": "4.6 Exercies",
    "text": "4.6 Exercies\nQuestions will be added later"
  }
]