<!DOCTYPE html><html>
<head>
<title>5 Choosing the smoothing parameter λ</title>
<!--Generated on Sat Mar  4 17:30:23 2023 by LaTeXML (version 0.8.4) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<link rel="stylesheet" href="ltx-ulem.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Choosing the smoothing parameter <math id="S5.m1" class="ltx_Math" alttext="\lambda" display="inline"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">The problem:</span> Suppose we are given data <math id="S5.I1.i1.p1.m1" class="ltx_Math" alttext="D=\{(t_{i},y_{i}),\,i=1,\ldots,n\}" display="inline"><semantics><mrow><mi>D</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo rspace="4.2pt">,</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">D=\{(t_{i},y_{i}),\,i=1,\ldots,n\}</annotation></semantics></math>.
Our model is:</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1" class="ltx_Math" alttext="y_{i}=f(t_{i})+\varepsilon_{i},\qquad\varepsilon_{i}\sim\text{N}(0,\sigma^{2})%
,\;i.i.d." display="block"><semantics><mrow><mrow><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msub><mi>ε</mi><mi>i</mi></msub></mrow></mrow><mo rspace="22.5pt">,</mo><mrow><msub><mi>ε</mi><mi>i</mi></msub><mo>∼</mo><mrow><mrow><mtext>N</mtext><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><mo rspace="5.3pt">,</mo><mi>i</mi></mrow></mrow></mrow><mo>.</mo><mi>i</mi><mo>.</mo><mi>d</mi></mrow><mo>.</mo></mrow><annotation encoding="application/x-tex">y_{i}=f(t_{i})+\varepsilon_{i},\qquad\varepsilon_{i}\sim\text{N}(0,\sigma^{2})%
,\;i.i.d.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.1)</span></td>
</tr>
</table>
<p class="ltx_p">where <math id="S5.I1.i1.p1.m2" class="ltx_Math" alttext="f(t)" display="inline"><semantics><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math> is smooth. Given knot positions <math id="S5.I1.i1.p1.m3" class="ltx_Math" alttext="\{t_{i},\;i=1,\dots,n\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><mrow><mrow><mrow><msub><mi>t</mi><mi>i</mi></msub><mo rspace="5.3pt">,</mo><mi>i</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t_{i},\;i=1,\dots,n\}</annotation></semantics></math>, we can estimate <math id="S5.I1.i1.p1.m4" class="ltx_Math" alttext="f(t)" display="inline"><semantics><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math> with a smoothing spline <math id="S5.I1.i1.p1.m5" class="ltx_Math" alttext="\hat{f}_{\lambda}(t)" display="inline"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo stretchy="false">^</mo></mover><mi>λ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{f}_{\lambda}(t)</annotation></semantics></math>.
</p>
</div>
<div id="S5.I1.i1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">How then should we choose the value of the smoothing parameter <math id="S5.I1.i1.p2.m1" class="ltx_Math" alttext="\lambda" display="inline"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>? By setting <math id="S5.I1.i1.p2.m2" class="ltx_Math" alttext="\lambda\rightarrow 0" display="inline"><semantics><mrow><mi>λ</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda\rightarrow 0</annotation></semantics></math>, we obtain exactly the interpolating spline <math id="S5.I1.i1.p2.m3" class="ltx_Math" alttext="\hat{f}_{0}(t)" display="inline"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo stretchy="false">^</mo></mover><mn>0</mn></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{f}_{0}(t)</annotation></semantics></math> and a perfect fit to the data. However, this tends to <em class="ltx_emph ltx_font_italic">overfit</em> the data: applying it to
to a new sample of data where model (<a href="#S5.E1" title="(5.1) ‣ item 1 ‣ 5 Choosing the smoothing parameter λ" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>) still applies would produce a poor fit.
Conversely, by setting <math id="S5.I1.i1.p2.m4" class="ltx_Math" alttext="\lambda\rightarrow\infty" display="inline"><semantics><mrow><mi>λ</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\lambda\rightarrow\infty</annotation></semantics></math>, we get:
</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex1.m1" class="ltx_Math" alttext="\displaystyle f_{\infty}(t)" display="inline"><semantics><mrow><msub><mi>f</mi><mi mathvariant="normal">∞</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle f_{\infty}(t)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex1.m2" class="ltx_Math" alttext="\displaystyle=\begin{cases}\bar{y},&amp;\nu=1,\ p=1\\
\hat{a}_{0}+\hat{a}_{1}t,&amp;\nu=2,\ p=3\end{cases}," display="inline"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd columnalign="left"><mrow><mover accent="true"><mi>y</mi><mo stretchy="false">¯</mo></mover><mo>,</mo></mrow></mtd><mtd columnalign="left"><mrow><mrow><mi>ν</mi><mo>=</mo><mn>1</mn></mrow><mo rspace="7.5pt">,</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mover accent="true"><mi>a</mi><mo stretchy="false">^</mo></mover><mn>0</mn></msub><mo>+</mo><mrow><msub><mover accent="true"><mi>a</mi><mo stretchy="false">^</mo></mover><mn>1</mn></msub><mo>⁢</mo><mi>t</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign="left"><mrow><mrow><mi>ν</mi><mo>=</mo><mn>2</mn></mrow><mo rspace="7.5pt">,</mo><mrow><mi>p</mi><mo>=</mo><mn>3</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\begin{cases}\bar{y},&amp;\nu=1,\ p=1\\
\hat{a}_{0}+\hat{a}_{1}t,&amp;\nu=2,\ p=3\end{cases},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S5.I1.i1.p2.m5" class="ltx_Math" alttext="\bar{y}=\sum_{i}y_{i}/n" display="inline"><semantics><mrow><mover accent="true"><mi>y</mi><mo stretchy="false">¯</mo></mover><mo>=</mo><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mi>i</mi></msub><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>/</mo><mi>n</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\bar{y}=\sum_{i}y_{i}/n</annotation></semantics></math> and <math id="S5.I1.i1.p2.m6" class="ltx_Math" alttext="\{\hat{a}_{0},\ \hat{a}_{1}\}" display="inline"><semantics><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>a</mi><mo stretchy="false">^</mo></mover><mn>0</mn></msub><mo rspace="7.5pt">,</mo><msub><mover accent="true"><mi>a</mi><mo stretchy="false">^</mo></mover><mn>1</mn></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\hat{a}_{0},\ \hat{a}_{1}\}</annotation></semantics></math> are
the OLS linear regression parameters. If the true <math id="S5.I1.i1.p2.m7" class="ltx_Math" alttext="f(t)" display="inline"><semantics><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math> was constant
or linear, this solution would be reasonable, but often we are
interested in less regular functions.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training and test data:</span> One way to approach estimation of
<math id="S5.I1.i2.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is to partition the set of indices <math id="S5.I1.i2.p1.m2" class="ltx_Math" alttext="I=\{1,\dots,n\}" display="inline"><semantics><mrow><mi>I</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">I=\{1,\dots,n\}</annotation></semantics></math> into
two subsets <math id="S5.I1.i2.p1.m3" class="ltx_Math" alttext="I_{1}" display="inline"><semantics><msub><mi>I</mi><mn>1</mn></msub><annotation encoding="application/x-tex">I_{1}</annotation></semantics></math> and <math id="S5.I1.i2.p1.m4" class="ltx_Math" alttext="I_{2}" display="inline"><semantics><msub><mi>I</mi><mn>2</mn></msub><annotation encoding="application/x-tex">I_{2}</annotation></semantics></math>, where <math id="S5.I1.i2.p1.m5" class="ltx_Math" alttext="I_{1}\cup I_{2}=S" display="inline"><semantics><mrow><mrow><msub><mi>I</mi><mn>1</mn></msub><mo>∪</mo><msub><mi>I</mi><mn>2</mn></msub></mrow><mo>=</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">I_{1}\cup I_{2}=S</annotation></semantics></math> and
<math id="S5.I1.i2.p1.m6" class="ltx_Math" alttext="I_{1}\cap I_{2}=\phi" display="inline"><semantics><mrow><mrow><msub><mi>I</mi><mn>1</mn></msub><mo>∩</mo><msub><mi>I</mi><mn>2</mn></msub></mrow><mo>=</mo><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">I_{1}\cap I_{2}=\phi</annotation></semantics></math>. Thus we obtain two datasets:</p>
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Training dataset: <math id="S5.I2.i1.p1.m1" class="ltx_Math" alttext="D_{1}=\{(t_{i},y_{i}),\ i\in I_{1}\}" display="inline"><semantics><mrow><msub><mi>D</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo rspace="7.5pt">,</mo><mi>i</mi><mo>∈</mo><msub><mi>I</mi><mn>1</mn></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">D_{1}=\{(t_{i},y_{i}),\ i\in I_{1}\}</annotation></semantics></math>,</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Test dataset: <math id="S5.I2.i2.p1.m1" class="ltx_Math" alttext="D_{2}=\{(t_{i},y_{i}),\ i\in I_{2}\}" display="inline"><semantics><mrow><msub><mi>D</mi><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo rspace="7.5pt">,</mo><mi>i</mi><mo>∈</mo><msub><mi>I</mi><mn>2</mn></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">D_{2}=\{(t_{i},y_{i}),\ i\in I_{2}\}</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p class="ltx_p">We fit a smoothing spline <math id="S5.I1.i2.p1.m7" class="ltx_Math" alttext="\hat{f}_{\lambda,I_{1}}(t)" display="inline"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo stretchy="false">^</mo></mover><mrow><mi>λ</mi><mo>,</mo><msub><mi>I</mi><mn>1</mn></msub></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{f}_{\lambda,I_{1}}(t)</annotation></semantics></math> to the training dataset,
and judge the quality of the fit using the test dataset:</p>
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S5.Ex2.m1" class="ltx_Math" alttext="\displaystyle Q_{I_{1}:I_{2}}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><msub><mi>I</mi><mn>1</mn></msub><mo>:</mo><msub><mi>I</mi><mn>2</mn></msub></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Q_{I_{1}:I_{2}}(\lambda)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S5.Ex2.m2" class="ltx_Math" alttext="\displaystyle=\sum_{i\in I_{2}}\left(y_{i}-\hat{f}_{\lambda,I_{1}}(t_{i})%
\right)^{2}." display="inline"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>∈</mo><msub><mi>I</mi><mn>2</mn></msub></mrow></munder></mstyle><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mover accent="true"><mi>f</mi><mo stretchy="false">^</mo></mover><mrow><mi>λ</mi><mo>,</mo><msub><mi>I</mi><mn>1</mn></msub></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{i\in I_{2}}\left(y_{i}-\hat{f}_{\lambda,I_{1}}(t_{i})%
\right)^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We choose <math id="S5.I1.i2.p1.m8" class="ltx_Math" alttext="\lambda" display="inline"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> to minimise <math id="S5.I1.i2.p1.m9" class="ltx_Math" alttext="Q_{I_{1}:I_{2}}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><msub><mi>I</mi><mn>1</mn></msub><mo>:</mo><msub><mi>I</mi><mn>2</mn></msub></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{I_{1}:I_{2}}(\lambda)</annotation></semantics></math>. Many
algorithms exist for such minimisation, for example through evaluation
on a fine grid of <math id="S5.I1.i2.p1.m10" class="ltx_Math" alttext="\lambda" display="inline"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> values, although many more computationally
efficient algorithms exist.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I2.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Cross-validation or ‘leave-one-out’:</span> This is an extreme
form of the above principle. The test dataset <math id="S5.I2.i3.p1.m1" class="ltx_Math" alttext="D_{2}" display="inline"><semantics><msub><mi>D</mi><mn>2</mn></msub><annotation encoding="application/x-tex">D_{2}</annotation></semantics></math> comprises a
single observation, <math id="S5.I2.i3.p1.m2" class="ltx_Math" alttext="(t_{j},y_{j})" display="inline"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(t_{j},y_{j})</annotation></semantics></math>, for a given value of <math id="S5.I2.i3.p1.m3" class="ltx_Math" alttext="j" display="inline"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>. The
training set <math id="S5.I2.i3.p1.m4" class="ltx_Math" alttext="D_{1}" display="inline"><semantics><msub><mi>D</mi><mn>1</mn></msub><annotation encoding="application/x-tex">D_{1}</annotation></semantics></math> is then
<math id="S5.I2.i3.p1.m5" class="ltx_Math" alttext="D_{-j}=\{(t_{i},y_{i}),\ i\in I_{-j}\}" display="inline"><semantics><mrow><msub><mi>D</mi><mrow><mo>-</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo rspace="7.5pt">,</mo><mi>i</mi><mo>∈</mo><msub><mi>I</mi><mrow><mo>-</mo><mi>j</mi></mrow></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">D_{-j}=\{(t_{i},y_{i}),\ i\in I_{-j}\}</annotation></semantics></math>, where
<math id="S5.I2.i3.p1.m6" class="ltx_Math" alttext="I_{-j}" display="inline"><semantics><msub><mi>I</mi><mrow><mo>-</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">I_{-j}</annotation></semantics></math> denotes the full set <math id="S5.I2.i3.p1.m7" class="ltx_Math" alttext="S" display="inline"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> excluding <math id="S5.I2.i3.p1.m8" class="ltx_Math" alttext="j" display="inline"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>. Then in a slightly amended
notation we can write</p>
<table id="S5.Ex3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex3.m1" class="ltx_Math" alttext="Q_{-j:j}(\lambda)=\left(y_{j}-\hat{f}_{\lambda,-j}(t_{j})\right)^{2}" display="block"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mrow><mo>-</mo><mi>j</mi></mrow><mo>:</mo><mi>j</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>-</mo><mrow><msub><mover accent="true"><mi>f</mi><mo stretchy="false">^</mo></mover><mrow><mi>λ</mi><mo>,</mo><mrow><mo>-</mo><mi>j</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">Q_{-j:j}(\lambda)=\left(y_{j}-\hat{f}_{\lambda,-j}(t_{j})\right)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">to assess the quality of fit. Of course, <math id="S5.I2.i3.p1.m9" class="ltx_Math" alttext="j" display="inline"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> is arbitrary, so we
repeat this process for each <math id="S5.I2.i3.p1.m10" class="ltx_Math" alttext="j\in\{1,\dots,n\}" display="inline"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">j\in\{1,\dots,n\}</annotation></semantics></math> then average the
assessments to form the <em class="ltx_emph ltx_font_italic">ordinary cross-validation criterion</em>:</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1" class="ltx_Math" alttext="Q_{OCV}(\lambda)=\frac{1}{n}\sum_{j=1}^{n}\left(y_{j}-\hat{f}_{\lambda,-j}(t_{%
j})\right)^{2}." display="block"><semantics><mrow><mrow><mrow><msub><mi>Q</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi><mo>⁢</mo><mi>V</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>-</mo><mrow><msub><mover accent="true"><mi>f</mi><mo stretchy="false">^</mo></mover><mrow><mi>λ</mi><mo>,</mo><mrow><mo>-</mo><mi>j</mi></mrow></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>.</mo></mrow><annotation encoding="application/x-tex">Q_{OCV}(\lambda)=\frac{1}{n}\sum_{j=1}^{n}\left(y_{j}-\hat{f}_{\lambda,-j}(t_{%
j})\right)^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5.2)</span></td>
</tr>
</table>
<p class="ltx_p">We then choose the value <math id="S5.I2.i3.p1.m11" class="ltx_Math" alttext="\hat{\lambda}" display="inline"><semantics><mover accent="true"><mi>λ</mi><mo stretchy="false">^</mo></mover><annotation encoding="application/x-tex">\hat{\lambda}</annotation></semantics></math> which minimises
<math id="S5.I2.i3.p1.m12" class="ltx_Math" alttext="Q_{OCV}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi><mo>⁢</mo><mi>V</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{OCV}(\lambda)</annotation></semantics></math>. Hopefully, a plot of <math id="S5.I2.i3.p1.m13" class="ltx_Math" alttext="Q_{OCV}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi><mo>⁢</mo><mi>V</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{OCV}(\lambda)</annotation></semantics></math> will
appear as in Figure <a href="#S5.F1" title="Figure 5.1 ‣ item 3 ‣ 5 Choosing the smoothing parameter λ" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, but there is no theoretical
guarantee that this curve will have a unique turning point, making
it difficult to locate the minimum.</p>
</div>
<figure id="S5.F1" class="ltx_figure"><img src="Figures/CplotCV.png" id="S5.F1.g1" class="ltx_graphics ltx_centering" width="213" height="213" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5.1: </span>Illustrating a cross-validation function <math id="S5.F1.m3" class="ltx_Math" alttext="Q_{OCV}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi><mo>⁢</mo><mi>V</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{OCV}(\lambda)</annotation></semantics></math> for choosing <math id="S5.F1.m4" class="ltx_Math" alttext="\hat{\lambda}" display="inline"><semantics><mover accent="true"><mi>λ</mi><mo stretchy="false">^</mo></mover><annotation encoding="application/x-tex">\hat{\lambda}</annotation></semantics></math>.</figcaption>
</figure>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I2.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Computation considerations:</span> At first sight, evaluation
of <math id="S5.I2.i4.p1.m1" class="ltx_Math" alttext="Q_{OCV}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi><mo>⁢</mo><mi>V</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{OCV}(\lambda)</annotation></semantics></math> for a given <math id="S5.I2.i4.p1.m2" class="ltx_Math" alttext="\lambda" display="inline"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> appears
computationally intensive: we must compute <math id="S5.I2.i4.p1.m3" class="ltx_Math" alttext="n" display="inline"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> different smoothing
solutions, each corresponding to one of the left-out data
points. Fortunately, there is a computational trick which enables
us to compute <math id="S5.I2.i4.p1.m4" class="ltx_Math" alttext="Q_{OCV}(\lambda)" display="inline"><semantics><mrow><msub><mi>Q</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi><mo>⁢</mo><mi>V</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{OCV}(\lambda)</annotation></semantics></math> directly from the smoothing spline
solution constructed from the whole dataset.
</p>
</div>
</li>
</ol>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Mar  4 17:30:23 2023 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
